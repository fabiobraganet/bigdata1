# ğŸï¸ DataLake Big Data Lab

Bem-vindo ao **DataLake Big Data Lab**! ğŸ‰ Este projeto Ã© um ambiente de laboratÃ³rio pronto para uso, projetado para fornecer uma plataforma de anÃ¡lise de big data usando Hadoop, Spark e Trino. Aqui, vocÃª encontrarÃ¡ instruÃ§Ãµes detalhadas sobre como configurar e usar esse ambiente incrÃ­vel.

## Ferramentas IncluÃ­das

- **[Apache Hadoop](https://hadoop.apache.org/)**: O backbone do nosso sistema de arquivos distribuÃ­dos e processamento em larga escala.
- **[Apache Spark](https://spark.apache.org/)**: O motor de processamento de dados rÃ¡pidos para anÃ¡lise em tempo real.
- **[Trino (Presto)](https://trino.io/)**: O motor de consulta SQL distribuÃ­do para explorar seus dados rapidamente.

## ğŸš€ Como Configurar

### PrÃ©-requisitos

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)

### Passo a Passo

1. **Clone o repositÃ³rio:**

```bash
git clone https://github.com/seu-usuario/lakehouse-big-data-lab.git
cd lakehouse-big-data-lab
```

2. **Configure o ambiente:**

   Certifique-se de que todas as variÃ¡veis de ambiente estÃ£o corretamente configuradas no arquivo `./hadoop/config/config`.

3. **Inicie o ambiente:**

```bash
docker-compose up -d
```

4. **Verifique se tudo estÃ¡ rodando:**

   - Acesse a interface do NameNode do Hadoop: [http://localhost:9870](http://localhost:9870)
   - Acesse a interface do ResourceManager do Hadoop: [http://localhost:8088](http://localhost:8088)
   - Acesse a interface do Spark Master: [http://localhost:8080](http://localhost:8080)
   - Acesse a interface do Spark Worker: [http://localhost:8081](http://localhost:8081)
   - Acesse a interface do Trino: [http://localhost:8082](http://localhost:8082)

## ğŸ› ï¸ Estrutura do Projeto

```plaintext
datalake-big-data-lab/
â”‚
â”œâ”€â”€ hadoop/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config
â”‚   â”œâ”€â”€ namenode/
â”‚   â””â”€â”€ datanode/
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ master/
â”‚   â””â”€â”€ worker/
â”‚
â”œâ”€â”€ trino/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ catalog/
â”‚   â””â”€â”€ data/
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ§° ConfiguraÃ§Ã£o dos ServiÃ§os

### Hadoop NameNode

O NameNode gerencia o armazenamento distribuÃ­do no HDFS.

```yaml
hadoop-namenode:
  image: apache/hadoop:3.3.5
  hostname: hadoop-namenode
  command: ["hdfs", "namenode"]
  ports:
    - 9870:9870 # Node Manager UI
  env_file:
    - ./hadoop/config/config
  environment:
    ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  volumes:
    - type: bind
      source: ./hadoop/namenode
      target: /etc/hadoop
  networks:
    - lakehouse-net
```

### Hadoop DataNode

O DataNode armazena os dados reais.

```yaml
hadoop-datanode:
  image: apache/hadoop:3.3.5
  command: ["hdfs", "datanode"]
  env_file:
    - ./hadoop/config/config
  networks:
    - lakehouse-net
```

### Hadoop ResourceManager

Gerencia recursos e alocaÃ§Ã£o de tarefas no cluster.

```yaml
hadoop-resourcemanager:
  image: apache/hadoop:3.3.5
  hostname: resourcemanager
  command: ["yarn", "resourcemanager"]
  ports:
    - 8088:8088 # Hadoop Cluster Manager UI
  env_file:
    - ./hadoop/config/config
  volumes:
    - ./test.sh:/opt/test.sh
  networks:
    - lakehouse-net
```

### Hadoop NodeManager

Executa tarefas no cluster.

```yaml
hadoop-nodemanager:
  image: apache/hadoop:3.3.5
  command: ["yarn", "nodemanager"]
  env_file:
    - ./hadoop/config/config
  networks:
    - lakehouse-net
```

### Spark Master

Gerencia o cluster Spark.

```yaml
spark-master:
  image: bitnami/spark:latest
  container_name: spark-master
  hostname: spark-master
  ports:
    - "8080:8080"  # Spark Web UI
    - "7077:7077"  # Spark Master
  environment:
    - SPARK_MODE=master
    - SPARK_MASTER_URL=spark://spark-master:7077
  volumes:
    - ./spark/master/data:/bitnami/spark
    - ./hadoop/config:/etc/hadoop/conf
  networks:
    - lakehouse-net
  depends_on:
    - hadoop-namenode
```

### Spark Worker

Executa tarefas de processamento distribuÃ­do.

```yaml
spark-worker:
  image: bitnami/spark:latest
  container_name: spark-worker
  hostname: spark-worker
  ports:
    - "8081:8081"  # Spark Worker UI
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
  volumes:
    - ./spark/worker/data:/bitnami/spark
    - ./hadoop/config:/etc/hadoop/conf
  networks:
    - lakehouse-net
  depends_on:
    - spark-master
```

### Trino

Motor de consulta SQL distribuÃ­do para grandes volumes de dados.

```yaml
trino:
  image: trinodb/trino:latest
  container_name: trino
  hostname: trino
  ports:
    - "8082:8080"  # Trino Web UI
  environment:
    - JAVA_TOOL_OPTIONS=-Dnode.environment=production -Dnode.id=trino -Dnode.data-dir=/data
  volumes:
    - ./trino/config:/etc/trino
    - ./trino/catalog:/etc/trino/catalog
    - ./trino/data:/data
  networks:
    - lakehouse-net
  depends_on:
    - hadoop-namenode
```

## ğŸ”— Links Ãšteis

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Apache Hadoop](https://hadoop.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Trino (Presto)](https://trino.io/)

## ğŸ“š Fontes

- [DocumentaÃ§Ã£o do Hadoop](https://hadoop.apache.org/docs/)
- [DocumentaÃ§Ã£o do Spark](https://spark.apache.org/docs/latest/)
- [DocumentaÃ§Ã£o do Trino](https://trino.io/docs/current/)

Divirta-se explorando o mundo dos dados! ğŸŒğŸ“ŠğŸš€
